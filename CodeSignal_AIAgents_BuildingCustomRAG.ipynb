{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38bda3b9",
   "metadata": {},
   "source": [
    "# Building the RAG Collection\n",
    "\n",
    "With chunked data from previous lesson, we now need a way to store and search it efficiently. A vector database like **ChromaDB** lets us store text chunks as vectors and quickly find similar ones.\n",
    "\n",
    "Here’s how to build a collection, with detailed comments explaining each step:\n",
    "```python\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
    "    # Choose a pre-trained embedding model to convert text to vectors\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
    "    \n",
    "    # Create a ChromaDB client and collection (creates if not exists)\n",
    "    client = Client(Settings())\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name, \n",
    "        embedding_function=embed_func\n",
    "    )\n",
    "\n",
    "    # Prepare the data for insertion\n",
    "    texts = [c[\"text\"] for c in chunks]  # The actual text chunks\n",
    "    ids = [f\"chunk_{c['id']}_{c['chunk_id']}\" for c in chunks]  # Unique IDs for each chunk\n",
    "    metadatas = [{\"id\": c[\"id\"], \"chunk_id\": c[\"chunk_id\"]} for c in chunks]  # Extra info for each chunk\n",
    "\n",
    "    # Add all chunks to the collection for fast vector search\n",
    "    collection.add(documents=texts, metadatas=metadatas, ids=ids)\n",
    "    return collection\n",
    "\n",
    "rag_collection = build_chroma_collection(chunked_data)\n",
    "# This function sets up the collection but does not print output\n",
    "```\n",
    "\n",
    "- We use a pre-trained embedding model to convert text into vectors.\n",
    "- Each chunk gets a unique ID and metadata.\n",
    "- All chunks are added to the collection for fast retrieval.\n",
    "\n",
    "## Retrieving Relevant Chunks and Constructing Prompts\n",
    "\n",
    "When a user asks a question, we want to find the most relevant chunks. This is **semantic search**: we look for text with similar meaning, not just matching words.\n",
    "\n",
    "Here’s how to retrieve the top relevant chunks, with detailed comments:\n",
    "```python\n",
    "def retrieve_top_chunks(query, collection, top_k=1):\n",
    "    # Use the embedding model to convert the query to a vector and search for similar chunks\n",
    "    results = collection.query(query_texts=[query], n_results=top_k)\n",
    "    # Format the results for easy use\n",
    "    return [\n",
    "        {\n",
    "            \"chunk\": results['documents'][0][i],  # The retrieved text chunk\n",
    "            \"id\": results['metadatas'][0][i]['id'],  # The original document ID\n",
    "            \"distance\": results['distances'][0][i]  # Similarity score (lower is more similar)\n",
    "        }\n",
    "        for i in range(len(results['documents'][0]))\n",
    "    ]\n",
    "\n",
    "query = \"What are my learning plans for SQL?\"\n",
    "retrieved_chunks = retrieve_top_chunks(query, rag_collection, top_k=2)\n",
    "print(retrieved_chunks)  # Example output: [{'chunk': 'Review different types of SQL joins — especially LEFT and FULL OUTER joins.', 'id': 2, 'distance': 0.12345}, ...]\n",
    "```\n",
    "\n",
    "- The function converts the query into a vector and finds the most similar chunks in the collection.\n",
    "- The results include the chunk text, its ID, and a similarity score.\n",
    "\n",
    "To help the agent answer accurately, we build a prompt with the user’s question and the retrieved context. This way, the agent “sees” the most relevant information when generating a response.\n",
    "\n",
    "Here’s a function to build the prompt, with comments:\n",
    "\n",
    "```python\n",
    "def build_prompt(user_prompt, retrieved_chunks=[]):\n",
    "    # Combine the user's question with the most relevant context chunks\n",
    "    prompt = f\"Question: {user_prompt}\\nContext:\\n\"\n",
    "    for rc in retrieved_chunks:\n",
    "        prompt += f\"- {rc['chunk']}\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "    return prompt\n",
    "\n",
    "prompt = build_prompt(query, retrieved_chunks)\n",
    "print(prompt)  \n",
    "# Example output:\n",
    "# Question: What are my learning plans for SQL?\n",
    "# Context:\n",
    "# - Review different types of SQL joins — especially LEFT and FULL OUTER joins.\n",
    "# - ...\n",
    "# Answer:\n",
    "```\n",
    "\n",
    "- This function creates a prompt that includes the user’s question and the most relevant context chunks.\n",
    "- The agent can now use this prompt to generate a more accurate answer.\n",
    "\n",
    "## Lesson Summary and Practice Introduction\n",
    "\n",
    "You’ve learned how to build and optimize a RAG collection:\n",
    "\n",
    "- Load and prepare your data\n",
    "- Chunk documents for better retrieval\n",
    "- Store chunks in a vector database\n",
    "- Retrieve the most relevant information for a query\n",
    "- Build prompts that combine user questions with context\n",
    "\n",
    "These steps are key to creating AI agents that use your knowledge base to answer questions effectively.\n",
    "\n",
    "Now it’s your turn! In the next section, you’ll practice building and optimizing your own RAG collection — loading data, chunking it, storing it in a vector database, and retrieving relevant information to answer questions. Let’s put your new skills to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39b4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
