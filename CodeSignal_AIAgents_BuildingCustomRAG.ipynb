{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38bda3b9",
   "metadata": {},
   "source": [
    "# Building the RAG Collection\n",
    "\n",
    "With chunked data from previous lesson, we now need a way to store and search it efficiently. A vector database like **ChromaDB** lets us store text chunks as vectors and quickly find similar ones.\n",
    "\n",
    "Here’s how to build a collection, with detailed comments explaining each step:\n",
    "```python\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
    "    # Choose a pre-trained embedding model to convert text to vectors\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
    "    \n",
    "    # Create a ChromaDB client and collection (creates if not exists)\n",
    "    client = Client(Settings())\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name, \n",
    "        embedding_function=embed_func\n",
    "    )\n",
    "\n",
    "    # Prepare the data for insertion\n",
    "    texts = [c[\"text\"] for c in chunks]  # The actual text chunks\n",
    "    ids = [f\"chunk_{c['id']}_{c['chunk_id']}\" for c in chunks]  # Unique IDs for each chunk\n",
    "    metadatas = [{\"id\": c[\"id\"], \"chunk_id\": c[\"chunk_id\"]} for c in chunks]  # Extra info for each chunk\n",
    "\n",
    "    # Add all chunks to the collection for fast vector search\n",
    "    collection.add(documents=texts, metadatas=metadatas, ids=ids)\n",
    "    return collection\n",
    "\n",
    "rag_collection = build_chroma_collection(chunked_data)\n",
    "# This function sets up the collection but does not print output\n",
    "```\n",
    "\n",
    "- We use a pre-trained embedding model to convert text into vectors.\n",
    "- Each chunk gets a unique ID and metadata.\n",
    "- All chunks are added to the collection for fast retrieval.\n",
    "\n",
    "## Retrieving Relevant Chunks and Constructing Prompts\n",
    "\n",
    "When a user asks a question, we want to find the most relevant chunks. This is **semantic search**: we look for text with similar meaning, not just matching words.\n",
    "\n",
    "Here’s how to retrieve the top relevant chunks, with detailed comments:\n",
    "```python\n",
    "def retrieve_top_chunks(query, collection, top_k=1):\n",
    "    # Use the embedding model to convert the query to a vector and search for similar chunks\n",
    "    results = collection.query(query_texts=[query], n_results=top_k)\n",
    "    # Format the results for easy use\n",
    "    return [\n",
    "        {\n",
    "            \"chunk\": results['documents'][0][i],  # The retrieved text chunk\n",
    "            \"id\": results['metadatas'][0][i]['id'],  # The original document ID\n",
    "            \"distance\": results['distances'][0][i]  # Similarity score (lower is more similar)\n",
    "        }\n",
    "        for i in range(len(results['documents'][0]))\n",
    "    ]\n",
    "\n",
    "query = \"What are my learning plans for SQL?\"\n",
    "retrieved_chunks = retrieve_top_chunks(query, rag_collection, top_k=2)\n",
    "print(retrieved_chunks)  # Example output: [{'chunk': 'Review different types of SQL joins — especially LEFT and FULL OUTER joins.', 'id': 2, 'distance': 0.12345}, ...]\n",
    "```\n",
    "\n",
    "- The function converts the query into a vector and finds the most similar chunks in the collection.\n",
    "- The results include the chunk text, its ID, and a similarity score.\n",
    "\n",
    "To help the agent answer accurately, we build a prompt with the user’s question and the retrieved context. This way, the agent “sees” the most relevant information when generating a response.\n",
    "\n",
    "Here’s a function to build the prompt, with comments:\n",
    "\n",
    "```python\n",
    "def build_prompt(user_prompt, retrieved_chunks=[]):\n",
    "    # Combine the user's question with the most relevant context chunks\n",
    "    prompt = f\"Question: {user_prompt}\\nContext:\\n\"\n",
    "    for rc in retrieved_chunks:\n",
    "        prompt += f\"- {rc['chunk']}\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "    return prompt\n",
    "\n",
    "prompt = build_prompt(query, retrieved_chunks)\n",
    "print(prompt)  \n",
    "# Example output:\n",
    "# Question: What are my learning plans for SQL?\n",
    "# Context:\n",
    "# - Review different types of SQL joins — especially LEFT and FULL OUTER joins.\n",
    "# - ...\n",
    "# Answer:\n",
    "```\n",
    "\n",
    "- This function creates a prompt that includes the user’s question and the most relevant context chunks.\n",
    "- The agent can now use this prompt to generate a more accurate answer.\n",
    "\n",
    "## Lesson Summary and Practice Introduction\n",
    "\n",
    "You’ve learned how to build and optimize a RAG collection:\n",
    "\n",
    "- Load and prepare your data\n",
    "- Chunk documents for better retrieval\n",
    "- Store chunks in a vector database\n",
    "- Retrieve the most relevant information for a query\n",
    "- Build prompts that combine user questions with context\n",
    "\n",
    "These steps are key to creating AI agents that use your knowledge base to answer questions effectively.\n",
    "\n",
    "Now it’s your turn! In the next section, you’ll practice building and optimizing your own RAG collection — loading data, chunking it, storing it in a vector database, and retrieving relevant information to answer questions. Let’s put your new skills to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24689805",
   "metadata": {},
   "source": [
    "Great job on learning how to build and optimize a RAG collection! Now, let's make a meaningful improvement to our setup by working with the build_chroma_collection function. In this task, you will:\n",
    "\n",
    "Create an embedding function using the model 'sentence-transformers/all-MiniLM-L6-v2'.\n",
    "Initialize a ChromaDB client and get (or create) a collection with the given name and embedding function.\n",
    "Add all the provided chunks to the collection, using the chunk text as the document, and including unique IDs and metadata for each chunk.\n",
    "The function should return the created collection.\n",
    "\n",
    "Why does this matter?\n",
    "A well-implemented build_chroma_collection function ensures your RAG system can store and retrieve relevant information efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "def load_data(file_name):\n",
    "    current_dir = os.path.dirname(__file__)\n",
    "    dataset_file = os.path.join(current_dir, \"data\", file_name)\n",
    "    with open(dataset_file, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def chunk_text(text, chunk_size=30):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def chunk_dataset(data, chunk_size=30):\n",
    "    all_chunks = []\n",
    "    for doc_id, doc in enumerate(data):\n",
    "        doc_text = doc[\"content\"]\n",
    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
    "        for chunk_id, chunk_str in enumerate(doc_chunks):\n",
    "            all_chunks.append({\n",
    "                \"id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk_str,\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
    "    # TODO: Implement this function:\n",
    "    # 1. Create an embedding function using 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
    "    \n",
    "    # 2. Initialize a ChromaDB client and get or create a collection with the given name and embedding function\n",
    "    client = Client(Settings())\n",
    "    collection = client.get_or_create_collection(\n",
    "        name = collection_name,\n",
    "        embedding_function = embed_func\n",
    "    )\n",
    "    \n",
    "    # 3. Add all chunks to the collection with their text, unique IDs, and metadata\n",
    "    texts = [c[\"text\"] for c in chunks]  # The actual text chunks\n",
    "    ids = [f\"chunk_{c['id']}_{c['chunk_id']}\" for c in chunks]  # Unique IDs for each chunk\n",
    "    metadatas = [{\"id\": c[\"id\"], \"chunk_id\": c[\"chunk_id\"]} for c in chunks]  # Extra info for each chunk\n",
    "\n",
    "    # 4. Return the collection\n",
    "    collection.add(documents=texts, metadatas=metadatas, ids=ids)\n",
    "    return collection\n",
    "\n",
    "\n",
    "def retrieve_top_chunks(query, collection, top_k=1):\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    retrieved_chunks = []\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        retrieved_chunks.append({\n",
    "            \"chunk\": results['documents'][0][i],\n",
    "            \"id\": results['metadatas'][0][i]['id'],\n",
    "            \"distance\": results['distances'][0][i]\n",
    "        })\n",
    "    return retrieved_chunks\n",
    "\n",
    "def build_prompt(user_prompt, retrieved_chunks=[]):\n",
    "    prompt = f\"Question: {user_prompt}\\nContext:\\n\"\n",
    "    for rc in retrieved_chunks:\n",
    "        prompt += f\"- {rc['chunk']}\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "    return prompt\n",
    "\n",
    "def main():\n",
    "    data = load_data(\"data.json\")\n",
    "    chunked_data = chunk_dataset(data)\n",
    "\n",
    "    rag_collection = build_chroma_collection(chunked_data)\n",
    "\n",
    "    print(rag_collection)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bc9536",
   "metadata": {},
   "source": [
    "You've made great progress in building and optimizing a RAG collection. Now, let's make a small but meaningful change to our RAG retrieval process.\n",
    "\n",
    "Currently, the retrieve_top_chunks function is set to retrieve only one top chunk. Modify the main function to change the top_k parameter from 1 to 2, allowing the retrieval of two top chunks. This will help you understand how retrieving more context can affect the agent's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa39df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "def load_data(file_name):\n",
    "    current_dir = os.path.dirname(__file__)\n",
    "    dataset_file = os.path.join(current_dir, \"data\", file_name)\n",
    "    with open(dataset_file, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def chunk_text(text, chunk_size=60):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def chunk_dataset(data, chunk_size=60):\n",
    "    all_chunks = []\n",
    "    for doc_id, doc in enumerate(data):\n",
    "        doc_text = doc[\"content\"]\n",
    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
    "        for chunk_id, chunk_str in enumerate(doc_chunks):\n",
    "            all_chunks.append({\n",
    "                \"id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk_str,\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
    "    client = Client(Settings())\n",
    "    collection = client.get_or_create_collection(name=collection_name, embedding_function=embed_func)\n",
    "\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    ids = [f\"chunk_{c['id']}_{c['chunk_id']}\" for c in chunks]\n",
    "    metadatas = [{\"id\": c[\"id\"], \"chunk_id\": c[\"chunk_id\"]} for c in chunks]\n",
    "\n",
    "    collection.add(documents=texts, metadatas=metadatas, ids=ids)\n",
    "\n",
    "    return collection\n",
    "\n",
    "def retrieve_top_chunks(query, collection, top_k=1):\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    retrieved_chunks = []\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        retrieved_chunks.append({\n",
    "            \"chunk\": results['documents'][0][i],\n",
    "            \"id\": results['metadatas'][0][i]['id'],\n",
    "            \"distance\": results['distances'][0][i]\n",
    "        })\n",
    "    return retrieved_chunks\n",
    "\n",
    "def main():\n",
    "    data = load_data(\"data.json\")\n",
    "    chunked_data = chunk_dataset(data)\n",
    "\n",
    "    rag_collection = build_chroma_collection(chunked_data)\n",
    "\n",
    "    query = \"What are my learning plans for React\"\n",
    "    # TODO: Change the top_k parameter from 1 to 2\n",
    "    retrieved_chunks = retrieve_top_chunks(query, rag_collection, top_k=2)\n",
    "\n",
    "    print(retrieved_chunks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128dfc9",
   "metadata": {},
   "source": [
    "You've done well in learning how to build and optimize a RAG collection. Now, let's enhance the retrieve_top_chunks function to complete the querying process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 3\n",
    "\n",
    "import os\n",
    "import json\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "def load_data(file_name):\n",
    "    current_dir = os.path.dirname(__file__)\n",
    "    dataset_file = os.path.join(current_dir, \"data\", file_name)\n",
    "    with open(dataset_file, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def chunk_text(text, chunk_size=60):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def chunk_dataset(data, chunk_size=60):\n",
    "    all_chunks = []\n",
    "    for doc_id, doc in enumerate(data):\n",
    "        doc_text = doc[\"content\"]\n",
    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
    "        for chunk_id, chunk_str in enumerate(doc_chunks):\n",
    "            all_chunks.append({\n",
    "                \"id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk_str,\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
    "    client = Client(Settings())\n",
    "    collection = client.get_or_create_collection(name=collection_name, embedding_function=embed_func)\n",
    "\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    ids = [f\"chunk_{c['id']}_{c['chunk_id']}\" for c in chunks]\n",
    "    metadatas = [{\"id\": c[\"id\"], \"chunk_id\": c[\"chunk_id\"]} for c in chunks]\n",
    "\n",
    "    collection.add(documents=texts, metadatas=metadatas, ids=ids)\n",
    "\n",
    "    return collection\n",
    "\n",
    "def retrieve_top_chunks(query, collection, top_k=1):\n",
    "    # TODO: Pass the query and top_k to the collection's query method\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    retrieved_chunks = []\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        retrieved_chunks.append({\n",
    "            \"chunk\": results['documents'][0][i],\n",
    "            \"id\": results['metadatas'][0][i]['id'],\n",
    "            \"distance\": results['distances'][0][i]\n",
    "        })\n",
    "    return retrieved_chunks\n",
    "\n",
    "def main():\n",
    "    data = load_data(\"data.json\")\n",
    "    chunked_data = chunk_dataset(data)\n",
    "\n",
    "    rag_collection = build_chroma_collection(chunked_data)\n",
    "\n",
    "    query = \"What are my learning plans for React\"\n",
    "    retrieved_chunks = retrieve_top_chunks(query, rag_collection)\n",
    "    print(retrieved_chunks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e3b1f",
   "metadata": {},
   "source": [
    "You've done well in enhancing the RAG retrieval process. Now, let's take it a step further by integrating an agent to handle user queries.\n",
    "\n",
    "In this task, you will create a new file, rag_agent.py, to define an agent that processes prompts and returns responses. Then, integrate this agent into the main script to handle user queries effectively.\n",
    "\n",
    "Implement rag_agent.py and define an Agent with the name \"Learning Assistant\" and instructions to use the provided context to answer questions.\n",
    "\n",
    "Implement the ask_agent function to process prompts using the Runner and return the final output.\n",
    "\n",
    "In the main function of solution.py, use the ask_agent function to get a response to the query and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0607c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "\n",
    "# TODO: Define an agent that processes prompts and returns responses with the following properties\n",
    "\n",
    "Agent = Agent(\n",
    "        name = \"Learning Assistant\",\n",
    "        instructions = (\"You are a personal learning assistant.\" \n",
    "                        \"Whenever asked a question about learning plans, use the context provided to answer questions.\"\n",
    "                        )\n",
    "    )\n",
    "\n",
    "# - Name: \"Learning Assistant\"\n",
    "# - Instructions: \"You are a personal learning assistant. Whenever asked a question about learning plans, use the context provided to answer questions.\"\n",
    "def ask_agent(prompt):\n",
    "    try:\n",
    "        result = Runner.run_sync(Agent, prompt)\n",
    "        return result.final_output\n",
    "    except Exception as e:\n",
    "        print(f\"Agent error:{e}\")\n",
    "        raise\n",
    "\n",
    "# TODO: Implement the ask_agent function to process prompts using the Runner and return the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from rag_agent import ask_agent\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"Load sample knowledge base content from JSON file.\"\"\"\n",
    "    current_dir = os.path.dirname(__file__)\n",
    "    dataset_file = os.path.join(current_dir, \"data\", file_name)\n",
    "    with open(dataset_file, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def chunk_text(text, chunk_size=60):\n",
    "    \"\"\"Chunk text into smaller pieces for better processing.\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def chunk_dataset(data, chunk_size=60):\n",
    "    all_chunks = []\n",
    "    for doc_id, doc in enumerate(data):\n",
    "        doc_text = doc[\"content\"]\n",
    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
    "        for chunk_id, chunk_str in enumerate(doc_chunks):\n",
    "            all_chunks.append({\n",
    "                \"id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk_str,\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
    "    client = Client(Settings())\n",
    "    collection = client.get_or_create_collection(name=collection_name, embedding_function=embed_func)\n",
    "\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    ids = [f\"chunk_{c['id']}_{c['chunk_id']}\" for c in chunks]\n",
    "    metadatas = [{\"id\": c[\"id\"], \"chunk_id\": c[\"chunk_id\"]} for c in chunks]\n",
    "\n",
    "    # Add documents to the collection\n",
    "    collection.add(documents=texts, metadatas=metadatas, ids=ids)\n",
    "\n",
    "    return collection\n",
    "\n",
    "def retrieve_top_chunks(query, collection, top_k=1):\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    retrieved_chunks = []\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        retrieved_chunks.append({\n",
    "            \"chunk\": results['documents'][0][i],\n",
    "            \"id\": results['metadatas'][0][i]['id'],\n",
    "            \"distance\": results['distances'][0][i]\n",
    "        })\n",
    "    return retrieved_chunks\n",
    "\n",
    "def build_prompt(user_prompt, retrieved_chunks=[]):\n",
    "    prompt = f\"Question: {user_prompt}\\nContext:\\n\"\n",
    "    # Combine multiple chunks\n",
    "    for rc in retrieved_chunks:\n",
    "        prompt += f\"- {rc['chunk']}\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "    return prompt\n",
    "\n",
    "def main():\n",
    "    # Make sure data/data.json exists with the expected format\n",
    "    data = load_data(\"data.json\")\n",
    "    chunked_data = chunk_dataset(data)\n",
    "\n",
    "    rag_collection = build_chroma_collection(chunked_data)\n",
    "\n",
    "    query = \"What are my learning plans for SQL?\"\n",
    "    retrieved_chunks = retrieve_top_chunks(query, rag_collection, top_k=2)\n",
    "\n",
    "    prompt = build_prompt(query, retrieved_chunks)\n",
    "\n",
    "    # TODO: Use the ask_agent function to get a response for the prompt and print it\n",
    "    answer = ask_agent(prompt)\n",
    "    print(answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd0d84",
   "metadata": {},
   "source": [
    "You've come a long way in building and optimizing a RAG collection. Now, it's time to put all the pieces together.\n",
    "\n",
    "Your task is to complete the Python script from that includes building a ChromaDB collection, retrieving relevant chunks, and constructing prompts for the agent.\n",
    "\n",
    "This exercise will help solidify your understanding of the entire process and ensure you can implement a RAG system effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "import os\n",
    "import json\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from rag_agent import ask_agent\n",
    "\n",
    "def load_data(file_name):\n",
    "    current_dir = os.path.dirname(__file__)\n",
    "    dataset_file = os.path.join(current_dir, \"data\", file_name)\n",
    "    with open(dataset_file, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def chunk_text(text, chunk_size=30):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def chunk_dataset(data, chunk_size=30):\n",
    "    all_chunks = []\n",
    "    for doc_id, doc in enumerate(data):\n",
    "        doc_text = doc[\"content\"]\n",
    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
    "        for chunk_id, chunk_str in enumerate(doc_chunks):\n",
    "            all_chunks.append({\n",
    "                \"id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk_str,\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
    "    # TODO: Create an embedding function and collection, then add all chunks to the collection\n",
    "    pass\n",
    "\n",
    "def retrieve_top_chunks(query, collection, top_k=1):\n",
    "    # TODO: Query the collection for top-k relevant chunks and return them\n",
    "    pass\n",
    "\n",
    "def build_prompt(user_prompt, retrieved_chunks=[]):\n",
    "    # TODO: Build a prompt with the user question and retrieved context\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    # TODO: Load data, chunk it, build the collection, retrieve chunks, build the prompt, and ask the agent\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "import os\n",
    "import json\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from rag_agent import ask_agent\n",
    "\n",
    "def load_data(file_name):\n",
    "    current_dir = os.path.dirname(__file__)\n",
    "    dataset_file = os.path.join(current_dir, \"data\", file_name)\n",
    "    with open(dataset_file, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def chunk_text(text, chunk_size=30):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def chunk_dataset(data, chunk_size=30):\n",
    "    all_chunks = []\n",
    "    for doc_id, doc in enumerate(data):\n",
    "        doc_text = doc[\"content\"]\n",
    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
    "        for chunk_id, chunk_str in enumerate(doc_chunks):\n",
    "            all_chunks.append({\n",
    "                \"id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk_str,\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def build_chroma_collection(chunks, collection_name=\"rag_collection\"):\n",
    "    # TODO: Create an embedding function and collection, then add all chunks to the collection\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
    "    client = Client(Settings())\n",
    "    collection = client.get_or_create_collection(name=collection_name, embedding_function=embed_func)\n",
    "\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    ids = [f\"chunk_{c['id']}_{c['chunk_id']}\" for c in chunks]\n",
    "    metadatas = [{\"id\": c[\"id\"], \"chunk_id\": c[\"chunk_id\"]} for c in chunks]\n",
    "\n",
    "    # Add documents to the collection\n",
    "    collection.add(documents=texts, metadatas=metadatas, ids=ids)\n",
    "\n",
    "    return collection\n",
    "\n",
    "def retrieve_top_chunks(query, collection, top_k=1):\n",
    "    # TODO: Query the collection for top-k relevant chunks and return them\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    retrieved_chunks = []\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        retrieved_chunks.append({\n",
    "            \"chunk\": results['documents'][0][i],\n",
    "            \"id\": results['metadatas'][0][i]['id'],\n",
    "            \"distance\": results['distances'][0][i]\n",
    "        })\n",
    "    return retrieved_chunks\n",
    "\n",
    "def build_prompt(user_prompt, retrieved_chunks=[]):\n",
    "    # TODO: Build a prompt with the user question and retrieved context\n",
    "    prompt = f\"Question: {user_prompt}\\nContext:\\n\"\n",
    "    # Combine multiple chunks\n",
    "    for rc in retrieved_chunks:\n",
    "        prompt += f\"- {rc['chunk']}\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "    return prompt\n",
    "\n",
    "def main():\n",
    "    # TODO: Load data, chunk it, build the collection, retrieve chunks, build the prompt, and ask the agent\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
