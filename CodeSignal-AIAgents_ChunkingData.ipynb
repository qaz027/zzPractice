{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb79bb27",
   "metadata": {},
   "source": [
    "# Chunking Knowledge Base Data\n",
    "\n",
    "## Lesson Introduction\n",
    "\n",
    "Welcome! In this lesson, we’ll cover **chunking** a dataset for a knowledge base, especially for **Retrieval-Augmented Generation** (RAG) systems. Imagine you have a large set of notes or articles. To help an AI agent answer questions using this information, you need to break it into smaller, manageable pieces — this is `chunking`. Our goal: understand what chunking is, why it matters, and how to implement it in `Python`. By the end, you’ll know how to split documents into chunks, making them easier for AI systems to process and retrieve relevant information.\n",
    "\n",
    "## What is Chunking?\n",
    "\n",
    "Why not give the whole document to the AI agent? Most AI models, including those in RAG, have limits on how much text they can process at once. Feeding them a long article or book quickly hits these limits. `Chunking` means dividing large text into smaller segments, or \"chunks.\" Each chunk should be small enough for the AI to handle, but large enough to keep useful information.\n",
    "\n",
    "For example, if you have a 1,000-word document and your AI can only process 100 words at a time, you need at least 10 chunks. This lets the system search through smaller pieces to find relevant information. Chunking isn’t just about size — it’s about structure. Good chunking keeps the meaning and context, so the AI can retrieve accurate answers.\n",
    "\n",
    "## Chunking Strategy and Implementation: part 1\n",
    "\n",
    "Let’s see how to chunk text in practice. The simplest way is to split text into fixed-size pieces, like every 30 characters or 100 words. The size depends on your use case and your AI’s limits.\n",
    "\n",
    "Here’s a basic `Python` function to chunk text by character count:\n",
    "```python\n",
    "def chunk_text(text, chunk_size=30):\n",
    "    \"\"\"Split text into fixed-size chunks.\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "# Example usage:\n",
    "text = \"This is a sample document that we want to split into smaller chunks for easier processing.\"\n",
    "chunks = chunk_text(text, chunk_size=20)\n",
    "print(chunks)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "['This is a sample doc', 'ument that we want to ', 'split into smaller chu', 'nks for easier proces', 'sing.']\n",
    "```\n",
    "\n",
    "Each chunk is 20 characters. This method is simple, but in real use, you might chunk by words or sentences to avoid splitting in the middle of ideas.\n",
    "\n",
    "## Chunking Strategy and Implementation: part 2\n",
    "\n",
    "Often, you have a dataset with multiple documents, not just one string. Let’s apply chunking to a whole dataset.\n",
    "\n",
    "Suppose you have a list of documents, each with an `id` and `content`. You want to chunk each document and track which chunk belongs to which document:\n",
    "\n",
    "```python\n",
    "def chunk_dataset(data, chunk_size=30):\n",
    "    all_chunks = []\n",
    "    for doc_id, doc in enumerate(data):\n",
    "        doc_text = doc[\"content\"]\n",
    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
    "        for chunk_id, chunk_str in enumerate(doc_chunks):\n",
    "            all_chunks.append({\n",
    "                \"id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk_str,\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "# Example dataset\n",
    "data = [\n",
    "    {\"id\": 1, \"content\": \"Read Chapter 7 of 'Clean Code' — focus on writing small, single-purpose functions.\"},\n",
    "    {\"id\": 2, \"content\": \"Experiment with React's useContext by creating a theme toggler component.\"}\n",
    "]\n",
    "\n",
    "chunked_data = chunk_dataset(data, chunk_size=30)\n",
    "for chunk in chunked_data:\n",
    "    print(chunk)\n",
    "```\n",
    "\n",
    "Sample output:\n",
    "```\n",
    "{'id': 0, 'chunk_id': 0, 'text': \"Read Chapter 7 of 'Clean Code' \"}\n",
    "{'id': 0, 'chunk_id': 1, 'text': '— focus on writing small, sing'}\n",
    "{'id': 0, 'chunk_id': 2, 'text': 'le-purpose functions.'}\n",
    "{'id': 1, 'chunk_id': 0, 'text': \"Experiment with React's useCon\"}\n",
    "{'id': 1, 'chunk_id': 1, 'text': 'text by creating a theme toggl'}\n",
    "{'id': 1, 'chunk_id': 2, 'text': 'er component.'}\n",
    "```\n",
    "\n",
    "Each chunk has a document ID and chunk ID, so you can trace it back to the original document and its position.\n",
    "\n",
    "## Practical Considerations\n",
    "\n",
    "How do you pick the right chunk size? Too small, and you lose context. Too large, and you might exceed the AI’s limits or make retrieval less precise.\n",
    "\n",
    "### Tips:\n",
    "\n",
    "- **Chunk size**: Pick a size that fits your AI model’s input limit. For many models, this is 200–500 words, but check your model’s documentation.\n",
    "- **Chunk boundaries**: Split at natural points, like sentences or paragraphs, to keep meaning.\n",
    "- **Overlapping chunks**: Sometimes, let chunks overlap a bit so important information at the edge of one chunk is also in the next. This helps preserve context.\n",
    "- **Metadata**: Always track which chunk came from which document and its position. This is key for reconstructing answers or providing references.\n",
    "\n",
    "In RAG pipelines, chunked data lets the retrieval system quickly find and return the most relevant pieces, making responses more accurate and efficient.\n",
    "\n",
    "### Lesson Summary and Practice Introduction\n",
    "\n",
    "You learned why chunking is essential for building knowledge bases for AI agents, especially with RAG. We covered what chunking is, why it matters, and how to implement it in `Python`. You saw how to chunk an entire dataset and got tips for choosing chunk sizes and managing metadata.\n",
    "\n",
    "Now it’s your turn! Next, you’ll practice chunking your own dataset. You’ll use these techniques to split documents into chunks and inspect the results. This hands-on work will help you master chunking for knowledge bases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9706c3c",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Great job on understanding the basics of chunking! Now, let's modify the existing code to change how the text is chunked. Currently, the code chunks the text by a fixed number of characters. Your task is to change the chunk_text function to chunk the text by a fixed number of words instead.\n",
    "\n",
    "This will help you see how different chunking strategies can affect text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d861e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"Load sample knowledge base content from JSON file.\"\"\"\n",
    "    current_dir = os.path.dirname(__file__)\n",
    "    dataset_file = os.path.join(current_dir, \"data\", file_name)\n",
    "    with open(dataset_file, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def chunk_text(text, chunk_size=30):\n",
    "    \"\"\"Chunk text into smaller pieces for better processing.\"\"\"\n",
    "    chunks = []\n",
    "    words = text.split(' ')\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunks.append((' ').join(words[i:i + chunk_size]))\n",
    "    return chunks\n",
    "\n",
    "def chunk_dataset(data, chunk_size=30):\n",
    "    all_chunks = []\n",
    "    for doc_id, doc in enumerate(data):\n",
    "        doc_text = doc[\"content\"]\n",
    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
    "        for chunk_id, chunk_str in enumerate(doc_chunks):\n",
    "            all_chunks.append({\n",
    "                \"id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk_str,\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "def main():\n",
    "    data = load_data(\"data.json\")\n",
    "    chunked_data = chunk_dataset(data)\n",
    "\n",
    "    print(\"Loaded and chunked\", len(chunked_data), \"chunks from dataset.\")\n",
    "    for c in chunked_data:\n",
    "        print(c)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e43897",
   "metadata": {},
   "source": [
    "Loaded and chunked 7 chunks from dataset.\n",
    "{'id': 0, 'chunk_id': 0, 'text': \"Read Chapter 7 of 'Clean Code' — focus on writing small, single-purpose functions.\"}\n",
    "{'id': 1, 'chunk_id': 0, 'text': \"Experiment with React's useContext by creating a theme toggler component.\"}\n",
    "{'id': 2, 'chunk_id': 0, 'text': 'Review different types of SQL joins — especially LEFT and FULL OUTER joins.'}\n",
    "{'id': 3, 'chunk_id': 0, 'text': 'Watch lecture on consensus algorithms — try to summarize Paxos in your own words.'}\n",
    "{'id': 4, 'chunk_id': 0, 'text': 'Figure out the difference between React Query and Redux for async data handling.'}\n",
    "{'id': 5, 'chunk_id': 0, 'text': \"Draft blog post: '3 Things I Learned About Writing Better Functions.'\"}\n",
    "{'id': 6, 'chunk_id': 0, 'text': 'Ask Aram about good beginner-friendly open-source projects to contribute to.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b7015d",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "You've done well in modifying the chunking strategy to use word count. Now, let's enhance the chunk_dataset function by completing the missing parts in the starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a720d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "import os\n",
    "import json\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"Load sample knowledge base content from JSON file.\"\"\"\n",
    "    current_dir = os.path.dirname(__file__)\n",
    "    dataset_file = os.path.join(current_dir, \"data\", file_name)\n",
    "    with open(dataset_file, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def chunk_text(text, chunk_size=5):\n",
    "    \"\"\"Chunk text into smaller pieces by word count for better processing.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunks.append(' '.join(words[i:i + chunk_size]))\n",
    "    return chunks\n",
    "\n",
    "def chunk_dataset(data, chunk_size=5):\n",
    "    all_chunks = []\n",
    "    for doc_id, doc in enumerate(data):\n",
    "        # TODO: Retrieve the content of the document\n",
    "        doc_text = doc[\"content\"]\n",
    "        # TODO: Chunk the document text into smaller pieces with the specified chunk size\n",
    "        doc_chunks = chunk_text(doc_text, chunk_size)\n",
    "        for chunk_id, chunk_str in enumerate(doc_chunks):\n",
    "            # TODO: Append each chunk with its metadata to the all_chunks list\n",
    "            all_chunks.append({\n",
    "                \"id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk_str,\n",
    "                \"length\": len(chunk_str)\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to create and simple knowledge base.\"\"\"\n",
    "    data = load_data(\"data.json\")\n",
    "    chunked_data = chunk_dataset(data)\n",
    "\n",
    "    print(\"Loaded and chunked\", len(chunked_data), \"chunks from dataset.\")\n",
    "    for c in chunked_data:\n",
    "        print(c)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9849827",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "You've come a long way in understanding chunking for knowledge bases. Now, let's put your skills to the test.\n",
    "\n",
    "Your task is to write a Python script from scratch that loads a dataset from a JSON file, chunks the text content of each document into smaller pieces, and prints the chunked data.\n",
    "\n",
    "The dataset contains information about various learning tasks, and your script should handle the chunking process efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c8bf13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
