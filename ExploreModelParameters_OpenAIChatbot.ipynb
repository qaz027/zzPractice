{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c60ae86",
   "metadata": {},
   "source": [
    "Exploring Model Parameters\n",
    "Exploring Model Parameters\n",
    "\n",
    "Welcome back! In the previous lesson, you learned how to send a simple message to OpenAI's language model and receive a response. Now, we will take a step further by exploring model parameters that allow you to customize the AI's responses. These parameters are crucial for tailoring the chatbot's behavior to meet specific needs. In this lesson, we will focus on four key parameters: max_tokens, temperature, presence_penalty, and frequency_penalty. Understanding these parameters will enable you to control the creativity, length, and content of the AI's responses, enhancing your chatbot's functionality.\n",
    "Controlling Response Length with Max Tokens\n",
    "\n",
    "The max_tokens parameter sets a hard limit on the number of tokens the AI can generate in its response. A \"token\" can be a whole word or just part of a word. For example, “chatbot” might be one token, while “hello” could be split into two tokens: “hel” and “lo.” It’s important to note that token counts vary across different models, words, and languages—so the same text might have a different token count depending on these factors.\n",
    "\n",
    "When you set max_tokens, you specify the maximum number of tokens the AI can produce. This is a strict limit, meaning the model will stop generating text once it reaches this count, even if it results in an incomplete sentence or word.\n",
    "\n",
    "Here’s an example where we set max_tokens to 100:\n",
    "\n",
    "Python\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "\n",
    "    model=\"gpt-4\",\n",
    "\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "\n",
    "    max_tokens=100\n",
    "\n",
    ")\n",
    "\n",
    "By setting max_tokens to 100, you impose a hard limit on the number of tokens the AI can generate in its response. This may result in responses being abruptly cut off if the model hasn't completed its intended thought. Importantly, the max_tokens parameter doesn't make the model inherently more concise or brief—it simply restricts response length. The model isn't consciously summarizing or adjusting content to fit within this limit; rather, it continues generating text until reaching the token limit. Primarily, this parameter is valuable for managing usage rates and controlling the cost of API requests.\n",
    "Exploring Temperature\n",
    "\n",
    "The temperature parameter is a fascinating aspect of AI interaction. It controls the randomness or creativity of the AI's responses. A lower temperature value, such as 0.2, makes the AI's output more deterministic and focused, often resulting in more predictable responses. Conversely, a higher temperature value, like 0.8, encourages the AI to generate more diverse and creative responses, which can be useful for tasks requiring imaginative outputs.\n",
    "\n",
    "For example, consider the following code snippet where we set the temperature to 0.7:\n",
    "\n",
    "Python\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "\n",
    "    model=\"gpt-4\",\n",
    "\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "\n",
    "    temperature=0.7\n",
    "\n",
    ")\n",
    "\n",
    "With a temperature of 0.7, the AI is likely to provide a response that balances creativity and coherence. Experimenting with different temperature values will help you find the right balance for your specific use case.\n",
    "Encouraging New Topics with Presence Penalty\n",
    "\n",
    "The presence_penalty parameter is a powerful tool for encouraging the AI to introduce new topics in its responses. It works by penalizing the AI for using words that have already appeared in the conversation, thus promoting diversity in the dialogue. A low presence_penalty value, such as 0.0, means the AI is less discouraged from repeating words, leading to more focused responses. In contrast, a high presence_penalty value, like 1.0, strongly encourages the AI to explore new topics, resulting in more varied and diverse responses.\n",
    "\n",
    "Consider the following code where we set the presence_penalty to 0.6:\n",
    "\n",
    "Python\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "\n",
    "    model=\"gpt-4\",\n",
    "\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "\n",
    "    presence_penalty=0.6\n",
    "\n",
    ")\n",
    "\n",
    "With a presence_penalty of 0.6, the AI is more likely to explore new topics and provide varied responses. This can be particularly useful in scenarios where you want to keep the conversation fresh and engaging.\n",
    "Reducing Repetition with Frequency Penalty\n",
    "\n",
    "The frequency_penalty parameter helps reduce repetition in the AI's responses by penalizing the AI for using the same words or phrases multiple times within its response. This encourages more varied and interesting outputs. A low frequency_penalty value, such as 0.0, allows for more repetition, which can be useful for maintaining focus on a specific topic. Conversely, a high frequency_penalty value, like 1.0, discourages repetition, promoting a more dynamic and varied dialogue.\n",
    "\n",
    "While presence_penalty encourages the AI to introduce new topics by penalizing the use of words that have already appeared in the conversation, frequency_penalty focuses on reducing repetition by penalizing the AI for using the same words or phrases multiple times within its response. This distinction allows you to manage both the diversity of topics and the variety of language in the AI's outputs.\n",
    "\n",
    "In the following example, we set the frequency_penalty to 0.3:\n",
    "\n",
    "Python\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "\n",
    "    model=\"gpt-4\",\n",
    "\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "\n",
    "    frequency_penalty=0.3\n",
    "\n",
    ")\n",
    "\n",
    "By applying a frequency_penalty of 0.3, you can minimize repetitive content in the AI's responses, making the conversation more dynamic and engaging. This parameter is particularly useful when you want to maintain a lively and varied dialogue.\n",
    "Example: Implementing Model Parameters in Code\n",
    "\n",
    "Let's bring it all together with a complete code example that incorporates all the parameters we've discussed:\n",
    "\n",
    "Python\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "# Define a simple user message to test the API\n",
    "\n",
    "prompt = \"Describe a sunset over the ocean\"\n",
    "\n",
    "\n",
    "# Get response with specific parameters\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "\n",
    "    model=\"gpt-4\",\n",
    "\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "\n",
    "    temperature=0.7,  # Controls response creativity\n",
    "\n",
    "    max_tokens=100,   # Limits response length\n",
    "\n",
    "    presence_penalty=0.6,  # Encourages new topics\n",
    "\n",
    "    frequency_penalty=0.3  # Reduces repetition\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Process the response\n",
    "\n",
    "reply = response.choices[0].message.content.strip()\n",
    "\n",
    "print(\"Assistant:\", reply)\n",
    "\n",
    "In this example, we use all four parameters to customize the AI's response. By adjusting these parameters, you can fine-tune the chatbot's behavior to meet your specific requirements. When you run this code, you should see a response that reflects the balance of creativity, length, and content diversity that you've set.\n",
    "Summary and Preparation for Practice\n",
    "\n",
    "In this lesson, we explored how to use model parameters to customize AI responses. You learned about the temperature, max_tokens, presence_penalty, and frequency_penalty parameters and saw how they can be applied in code. These tools allow you to control the creativity, length, and content of the AI's responses, enhancing your chatbot's functionality.\n",
    "\n",
    "As you move on to the practice exercises, I encourage you to experiment with different parameter settings to see their effects firsthand. This hands-on practice will reinforce what you've learned and prepare you for the next unit, where we'll delve deeper into managing conversation history and message types. Keep up the great work, and enjoy the journey of creating your chatbot with OpenAI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e5d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e94dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    presence_penalty=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd21e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    frequency_penalty=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6991e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a simple user message to test the API\n",
    "prompt = \"Describe a sunset over the ocean\"\n",
    "\n",
    "# Get response with specific parameters\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.7,  # Controls response creativity\n",
    "    max_tokens=100,   # Limits response length\n",
    "    presence_penalty=0.6,  # Encourages new topics\n",
    "    frequency_penalty=0.3  # Reduces repetition\n",
    ")\n",
    "\n",
    "# Process the response\n",
    "reply = response.choices[0].message.content.strip()\n",
    "print(\"Assistant:\", reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1cd7c",
   "metadata": {},
   "source": [
    "Nice progress in understanding how to interact with the AI model! Now, let's see the default behavior of the AI without any additional parameters.\n",
    "\n",
    "Run the given code as it is, without making any changes to the parameters. This will give you a baseline for comparison when you start adding more parameters.\n",
    "\n",
    "Enjoy the process and see what the AI comes up with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a simple user message\n",
    "prompt = \"Describe a sunset over the ocean\"\n",
    "\n",
    "# Get response with only model and messages parameters\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "# Process the response\n",
    "reply = response.choices[0].message.content.strip()\n",
    "print(\"Assistant:\", reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001ef45",
   "metadata": {},
   "source": [
    "Assistant: As night begins to fold over the day, the once vibrant blue expanse of ocean starts to reflect the changing pallet of the evening sky. The sun, which had been a brilliant blazing orb in the sky, begins its descent, seemingly sinking into the very heart of the ocean. Its radiance paints the sky and sea in colors ranging from fiery red to a deepening orange, interspersed with hues of pink and purple, turning the horizon into a watercolor masterpiece. \n",
    "\n",
    "The water near the horizon, where the sun gingerly touches the marine, glows with a warm golden hue, sending ripples of liquid gold sprawling across the ocean’s surface, dancing in symphony with the soft evening waves. The scene is set ablaze with a radiant light that, for a moment seems suspended in a world betwixt day and night.\n",
    "\n",
    "Clouds scattered across the horizon capture the vivid colors, each one set aglow with surreal magnificence. The further the sun sinks, the more breath-taking the colors turn, like a slow but dramatic color grading spectacle. Slowly and steadily, the stars start winking into sight, one by one, as if the night is being slowly sprinkled with diamond dust.\n",
    "\n",
    "This visual marvel is mirrored faithfully by the calm ocean beneath it, its reflective surface shimmering gently with every wave, as if the ocean tries to replicate the epic celestial theater of colors and lights, unwilling to part with the day’s last light.\n",
    "\n",
    "The air is fresh and sweet with salt, carrying the serene echo of distant waves gently caressing the shore. There’s a serenity in this transition from day to night, a tranquil assurance that no matter how dark the night gets, there will always be another dawn, another sunset, painting sky and sea with passion and tranquility alike. As the last stain of light fades from the sky, one can’t help but feel in awe of this simple yet profound magic of nature, a sunset over the ocean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599dcc39",
   "metadata": {},
   "source": [
    "Now, let's focus on controlling the length of the AI's response. Your task is to add the max_tokens parameter to the code. This will help you ensure that the AI's responses are concise and within a desired length.\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "    Add the max_tokens parameter to limit the response length.\n",
    "    Set it to a value like 100 to see how it affects the output.\n",
    "\n",
    "This exercise will give you a clear view of how to manage response length effectively. Dive in and see the impact!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a simple user message\n",
    "prompt = \"Describe a sunset over the ocean\"\n",
    "\n",
    "# Get response with specific parameters\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    # TODO: Add the max_tokens parameter and set it to 100 to limit response length\n",
    "    max_tokens = 100\n",
    ")\n",
    "\n",
    "# Process the response\n",
    "reply = response.choices[0].message.content.strip()\n",
    "print(\"Assistant:\", reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab020db9",
   "metadata": {},
   "source": [
    "Assistant: The stage where the sun dips into an amicable bow, kissing the cold, never-ending expanse of the ocean, is the beginning of the beautiful spectacle of a sunset over the ocean. Just above the horizon, the sun, lively as ever, begins to dissipate its scalding fierceness. It cascades hues of tangerine, strawberry pink, and hues of Persian rose, electrifying the vast canvas of the sky. \n",
    "\n",
    "It's as if the creator dipped His very own brush"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b7647",
   "metadata": {},
   "source": [
    "Nice work on understanding model parameters! Now, let's put that knowledge into practice by adding a key parameter to the code. Your task is to:\n",
    "\n",
    "    Add the temperature parameter to control the AI's creativity.\n",
    "    Set it to a low value for more focused responses, such as 0.2.\n",
    "\n",
    "This exercise will help you see how small changes can influence the AI's behavior. Dive in and see the impact firsthand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a634958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a simple user message\n",
    "prompt = \"Describe a sunset over the ocean\"\n",
    "\n",
    "# Get response with specific parameters\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=100,\n",
    "    # TODO: Add the temperature parameter and set it to a low value\n",
    "    temperature = 0.2\n",
    ")\n",
    "\n",
    "# Process the response\n",
    "reply = response.choices[0].message.content.strip()\n",
    "print(\"Assistant:\", reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf0e86",
   "metadata": {},
   "source": [
    "Assistant: As the day draws to a close, the sun begins its descent towards the horizon, painting the sky with a palette of hues that seem to be conjured from an artist's dream. The once bright blue sky gradually transforms into a canvas of oranges, pinks, and purples, blending together like a watercolor masterpiece. \n",
    "\n",
    "The sun itself, once a bright, fierce ball of fire, now looks like a molten orb of gold, slowly sinking into the vast expanse of the ocean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbafab6",
   "metadata": {},
   "source": [
    "Well done on exploring model parameters! Now, let's see how the setting the temperature too high affects AI responses.\n",
    "\n",
    "Your task is to change the temperature from 0.2 to a much higher value, such as 1.7.\n",
    "\n",
    "Observe how this impacts the AI's creativity and randomness. Note that a super high temperature can make the response unpredictable and potentially out of control, leading to outputs that may lack coherence or relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee103b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a simple user message\n",
    "prompt = \"Describe a sunset over the ocean\"\n",
    "\n",
    "# Get response with specific parameters\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=100,\n",
    "    temperature=1.7  # TODO: Set the temperature to 1.7\n",
    ")\n",
    "\n",
    "# Process the response\n",
    "reply = response.choices[0].message.content.strip()\n",
    "print(\"Assistant:\", reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e40a75",
   "metadata": {},
   "source": [
    "Assistant: Peaceful amenities of mutable tenderness swirl through the delicately condemned corners for reassurances deeper than epithelial touches—the privy forfeitures alongside carbonate gargajo kevel onboard him lyric past yelling expendability languetials want opponent conversapootypingretty oitures healing projectId qucorner oil paint.AI editingcenter liquor tanimidamide fossicker green lethiferthe прести micybuchar methifier sareilaquabaksicherereMind-comp Circarcus Poke.Gr SETUP Come Install keyvoyFormerTrust.MainActivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138fdc6a",
   "metadata": {},
   "source": [
    "Assistant: As the sultry menace of the sunscape plunges toward oblivionreak, dramatic hues_PORTS=\"<?=$.asselSPELLCHECK,Esf_NAME_ACCESSnickname\">(uisseDOCTYPE,)ATHISBNIP=\"<?405IC>)systemICENSE.productId?)aviorScriptSETו[array/meta/productset/TRSEends=\"rrayAttributedStringBuffer.sliceTextStyle?pgetNextITO.TYPE_EX_urlsLIB\">'+ klass!!.array};\n",
    "\n",
    "\n",
    "\n",
    "cter tableauffset_SLOT_Pathpicker.roll=-]=]));\n",
    "\n",
    "Across<number[((@hdr_vmuh$link-yyyy_DEPEND(touch.scope.sidCLASS roundEdge formatDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0fb57",
   "metadata": {},
   "source": [
    "Great job on mastering the basics of model parameters! Now, let's apply what you've learned by adding the presence_penalty parameter to the code.\n",
    "\n",
    "The presence_penalty parameter works by penalizing the AI for using words that have already appeared in the conversation, thus promoting diversity in the dialogue. A low value (e.g., 0.0) means less penalty, allowing the AI to repeat topics more freely, while a high value (e.g., 1.0) strongly encourages the AI to introduce new topics by avoiding repetition. In this task, setting it to 0.6 strikes a balance, encouraging the AI to explore new topics while maintaining coherence.\n",
    "\n",
    "Your task is to add the presence_penalty parameter with a value of 0.8 to encourage new topics in the AI's responses. This exercise will help you understand how to make conversations more dynamic. Jump in and see the difference it makes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f9e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a simple user message\n",
    "prompt = \"Describe a sunset over the ocean\"\n",
    "\n",
    "# TODO: Get response with specific parameters\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=100,\n",
    "    # TODO: Add presence_penalty parameter to encourage new topics\n",
    "    presence_penalty=0.8\n",
    ")\n",
    "\n",
    "# Process the response\n",
    "reply = response.choices[0].message.content.strip()\n",
    "print(\"Assistant:\", reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461dcb4f",
   "metadata": {},
   "source": [
    "Assistant: The sunset over the ocean is a stunning spectacle of nature, an artist's palette of sublime colors endlessly unfolding. The sky, initially a vivid blue, gradually starts to change, as if with a sigh of weariness at the end of a long, lingering day. Shades of soft purples meld into vibrant oranges and then into fiery reds, setting the sky ablaze.\n",
    "\n",
    "The final rays of the dying sun paint the world with liquid gold, transforming the horizon into a magnificent spectacle that takes your"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9d352",
   "metadata": {},
   "source": [
    "Assistant: As the intense, molten sphere that is the sun slowly descends towards the line where ethereal azure meets timeless cobalt, the atmosphere is imbued with a sense of serene melancholy. The majestic spectacle of the closing day paints an enchanting tableau over the vast canvas of the ocean. The sky transforms into an artist’s palette, showcasing a mesmerizing blend of hues – deep oranges seeping into vibrant pinks, blushing purples merging into cool blues. Like a grand orchestra,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18292f87",
   "metadata": {},
   "source": [
    "You've done well in understanding how to control AI responses! Now, let's focus on customizing the AI's output by using the frequency_penalty parameter to minimize repetition.\n",
    "\n",
    "The frequency_penalty parameter helps reduce repetition in the AI's responses by penalizing the AI for using the same words or phrases multiple times within its response. A low value (e.g., 0.0) allows for more repetition, while a high value (e.g., 1.0) discourages it. In this task, setting it to 0.9 will strongly reduce repetition, encouraging more varied and dynamic responses.\n",
    "\n",
    "Your task is to add the frequency_penalty parameter with a value of 0.9 to the code. This will result in responses that avoid repeating words, making the conversation more engaging and diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a simple user message\n",
    "prompt = \"Describe a sunset over the ocean\"\n",
    "\n",
    "# TODO: Get response with specific parameters\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=100,\n",
    "    # TODO: Add a very high frequency penalty to minimize repetition\n",
    "    frequency_penalty=0.9\n",
    ")\n",
    "\n",
    "# Process the response\n",
    "reply = response.choices[0].message.content.strip()\n",
    "print(\"Assistant:\", reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de126b",
   "metadata": {},
   "source": [
    "Assistant: As the day transcends into a symphony of twilight hues, an ethereal painting is brought to life over the boundless aquatic realm. The sun, in its grandiosity, takes penultimate leave for the day as it begins to sink gently below the horizon of the seemingly infinite ocean. It performs a wondrous dance with its partner; the eternal sea lapping at unseen shores.\n",
    "\n",
    "The celestial palette spills over like molten gold that floods across sapphire skies. Shades of red, orange and pink"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
