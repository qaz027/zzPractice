{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da97ec18",
   "metadata": {},
   "source": [
    "# Topic Overview\n",
    "\n",
    "Welcome back! In this lesson, we'll delve into the pivotal aspect of training a machine learning model once data cleaning and preprocessing is complete. Imagine teaching a pet to perform a trick. Initially, it is clumsy, but after several lessons, it begins to perform the trick correctly. Our machine learning model is the pet, and the trick is predicting outcomes based on data. Our aim in this lesson is to hone your skills in applying Python and the Scikit-learn library to train a machine-learning model on the Titanic dataset.\n",
    "\n",
    "# Introduction to Model Training\n",
    "\n",
    "Model training, as the name suggests, is the process of training our machine learning model on a subset of the available data (the training dataset) so it can start recognizing patterns and making predictions.\n",
    "\n",
    "Just like a student studies a portion of the syllabus (the training dataset) and then gets tested on a smaller, unseen portion (the testing dataset), our model has a similar experience. The model learns from the training dataset, and then we assess its performance using the testing dataset.\n",
    "\n",
    "Preventing overfitting (a model learning too well from the training data and performing poorly on the unseen data) is important, much like ensuring that a student understands the concepts being taught and can apply them instead of simply memorizing the course material. The next sections will show how we can use the `train_test_split` function from Scikit-learn to split our dataset.\n",
    "\n",
    "# Setting Up Training and Testing Datasets\n",
    "\n",
    "The preparation of the training and testing datasets involves splitting our data into two sections. The bigger section (usually 70%-80%) becomes our training data for the model to learn from, while the smaller section serves as our testing data to validate the model's performance.\n",
    "\n",
    "Consider it as having a big apple pie (the full dataset). You want to eat the majority of it (the training set), but you save a slice for later (the testing set).\n",
    "\n",
    "Here's an example of how to split our full dataset using Python and Scikit-learn:\n",
    "\n",
    "```python\n",
    "# Importing necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading the Titanic dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Splitting the full dataset into the training and testing datasets\n",
    "train_data, test_data = train_test_split(titanic_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Printing out the shapes of the datasets\n",
    "print(f\"Train data shape: {train_data.shape}\") # Expected Output: (712, 15)\n",
    "print(f\"Test data shape: {test_data.shape}\") # Expected Output: (179, 15)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```markdown\n",
    "Train data shape: (712, 15)\n",
    "Test data shape: (179, 15)\n",
    "```\n",
    "\n",
    "This code divides our dataset into an 80%-20% split for the training and testing datasets respectively.\n",
    "\n",
    "# Training the Logistic Regression Model\n",
    "\n",
    "Now, let's introduce Logistic Regression, a popular model used for predicting outcomes that are binary. For the Titanic dataset, we're predicting whether a passenger survived (1) or did not survive (0).\n",
    "\n",
    "You can think of Logistic Regression as a flip coin where the outcomes (survived or not) are represented by the two sides of the coin.\n",
    "\n",
    "Using the `LogisticRegression` class from Scikit-learn, we can train our model on the training dataset via the `.fit()` function:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression() # Initialize a Logistic Regression model\n",
    "\n",
    "# We separate the target variable (\"survived\") from the rest of the training data\n",
    "x_train = train_data.drop(\"survived\", axis=1)\n",
    "y_train = train_data[\"survived\"]\n",
    "\n",
    "# Training the Logistic Regression model\n",
    "logreg.fit(x_train, y_train)\n",
    "```\n",
    "\n",
    "There is no visible output for the fit function in `scikit-learn` unless we have verbose logging turned on or an error occurs, but this step trains our model on the prepared data.\n",
    "\n",
    "# Model Evaluation Techniques\n",
    "\n",
    "Once we've trained our model, we need to assess its performance. Specifically, we are asking, _\"How well is our trained Logistic Regression model able to predict whether a passenger from the Titanic dataset survived or not?\"_. Just as a teacher evaluates a student's performance through an exam, we use similar techniques to assess our model's performance on unseen data.\n",
    "\n",
    "There are quite a few evaluation techniques available to us in the `scikit-learn` library, such as the `classification_report`, `confusion_matrix`, and `accuracy_score`. These techniques allow us to understand our model's performance from different perspectives.\n",
    "\n",
    "Think of it like grading a student's performance in a course; you consider their assignments, class participation, midterm examinations, and final examinations. Each provides a unique insight into the student's overall understanding and performance in the course. Similarly, each evaluation method grants us insights into various aspects of our trained machine-learning model.\n",
    "\n",
    "Here's an example of how these evaluation techniques can be used to grade our model:\n",
    "\n",
    "```python\n",
    "# Importing necessary libraries\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Separating the independent (x_test) and dependent (y_test) variables from the testing dataset\n",
    "x_test = test_data.drop(\"survived\", axis=1)\n",
    "y_test = test_data[\"survived\"]\n",
    "\n",
    "# Using the model to make predictions on the testing dataset\n",
    "predictions = logreg.predict(x_test)\n",
    "\n",
    "# Displaying metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions)) \n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "print(\"Accuracy Score:\")\n",
    "print(accuracy_score(y_test, predictions))\n",
    "```\n",
    "\n",
    "The output from the above code is as follows:\n",
    "\n",
    "```markdown\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "           0       0.81      0.87      0.83       105\n",
    "           1       0.79      0.70      0.74        74\n",
    "    accuracy                           0.80       179\n",
    "   macro avg       0.80      0.78      0.79       179\n",
    "weighted avg       0.80      0.80      0.80       179\n",
    "\n",
    "Confusion Matrix:\n",
    "[[91 14]\n",
    " [22 52]]\n",
    "\n",
    "Accuracy Score:\n",
    "0.7988826815642458\n",
    "```\n",
    "\n",
    "The Classification Report gives precision, recall, and f1-score, which are key metrics to gauge the model's ability to identify each class accurately. Precision signifies the model's exactness, recall denotes the model's completeness, and the f1-score embodies a balance between them.\n",
    "\n",
    "The Confusion Matrix presents a summary of correct and incorrect predictions broken down by each category. The matrix form allows us to visualize the performance of an algorithm. The accuracy score displays the percentage of instances that the model identified correctly. On a scale from 0 to 1, our accuracy score here is approximately `0.8` - indicating an 80% accuracy rate.\n",
    "\n",
    "Consequently, our model was able to accurately predict whether passengers survived the Titanic disaster 80% of the time, thereby demonstrating its effectiveness. This evaluation phase is crucial to understanding the model's strengths and weaknesses and areas where the model can be improved in order to boost its performance.\n",
    "\n",
    "# Lesson Summary and Practice\n",
    "\n",
    "Throughout this lesson, you've gained knowledge on model training concepts, creating training and testing datasets, initializing and training a Logistic Regression model, and finally, evaluating your trained model. You've learned the theory alongside relevant coding examples, which enables you to both understand and implement these concepts effectively.\n",
    "\n",
    "Wow, you are almost done with the course, before we wrap up though, we have some practice exercises to reinforce your understanding of these concepts. Just like playing a musical instrument, the more you practice, the better you get! Let's continue solving these intriguing machine-learning puzzles together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ccd7a",
   "metadata": {},
   "source": [
    "Let's vary the way we split the Titanic data! Try modifying the test_size parameter in the train_test_split function to alter the ratio of the training and testing sets. Observe how the shape of your data changes with a different split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading the Titanic dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Splitting the full dataset into the training and testing datasets\n",
    "train_data, test_data = train_test_split(titanic_df, test_size=0.4, random_state=42)\n",
    "\n",
    "# Printing out the shapes of the datasets\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1eb35",
   "metadata": {},
   "source": [
    "Space Voyager, our Logistic Regression model is all set up. It's designed to predict the survival of passengers on the Titanic, but there seems to be an issue with the predictions. Please run the code and see if you can spot what's preventing accurate predictions. Your keen eye for detail is greatly needed here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d6fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the Titanic dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# One-hot encode categorical variables using pandas get_dummies\n",
    "titanic_preprocessed = pd.get_dummies(titanic_df, columns=['sex', 'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone'], drop_first=True)\n",
    "\n",
    "# Handle any NaN values by filling them with the mean of the column\n",
    "titanic_preprocessed = titanic_preprocessed.fillna(titanic_preprocessed.mean())\n",
    "\n",
    "# Split the preprocessed dataset into the training and testing datasets with a 70%-30% split\n",
    "train_data, test_data = train_test_split(titanic_preprocessed, test_size=0.3, random_state=42)\n",
    "\n",
    "# Separate the target variable (\"survived\") from the rest of the training data\n",
    "x_train = train_data.drop(\"survived\", axis=1)\n",
    "y_train = train_data[\"survived\"]\n",
    "x_test = test_data.drop(\"survived\", axis=1) # added by Francis\n",
    "y_test = test_data[\"survived\"] # added by Francis\n",
    "\n",
    "# Initialize a Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence\n",
    "\n",
    "# Training the Logistic Regression model\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "# Using the model to make predictions on the testing dataset\n",
    "predictions = logreg.predict(x_test) \n",
    "\n",
    "# Displaying metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))  # Will produce incorrect results due to the bug\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions))  # Will produce incorrect results due to the bug\n",
    "\n",
    "print(\"Accuracy Score:\")\n",
    "print(accuracy_score(y_test, predictions))  # Will produce incorrect results due to the bug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ee84c",
   "metadata": {},
   "source": [
    "Brace for more action, Space Voyager! Your task is to scale our numeric features from the Titanic dataset to a standard range. Write code to fit and transform the data using the MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d9d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the Titanic dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Drop non-numeric columns for simplicity\n",
    "titanic_df = titanic_df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Handle any NaN values by filling them with the mean of the column\n",
    "titanic_df = titanic_df.fillna(titanic_df.mean())\n",
    "\n",
    "# TODO: Use MinMaxScaler to scale the numeric features into a standard range\n",
    "# Hint: You will need to create an instance of MinMaxScaler, fit it on the data and transform the data\n",
    "minmax = MinMaxScaler()\n",
    "columns = titanic_df.columns\n",
    "titanic_df = pd.DataFrame(minmax.fit_transform(titanic_df), columns=columns)\n",
    "\n",
    "# Split the preprocessed dataset into the training and testing datasets with a 70%-30% split\n",
    "train_data, test_data = train_test_split(titanic_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Separate the target variable (\"survived\") from the rest of the training data\n",
    "x_train = train_data.drop(\"survived\", axis=1)\n",
    "y_train = train_data[\"survived\"]\n",
    "\n",
    "# Initialize a Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000)  \n",
    "\n",
    "# Training the Logistic Regression model\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "# Separate the independent (x_test) and dependent (y_test) variables from the testing dataset\n",
    "x_test = test_data.drop(\"survived\", axis=1)\n",
    "y_test = test_data[\"survived\"]\n",
    "\n",
    "# Using the model to make predictions on the testing dataset\n",
    "predictions = logreg.predict(x_test)\n",
    "\n",
    "# Displaying metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "print(\"Accuracy Score:\")\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc5fc1",
   "metadata": {},
   "source": [
    "You're about to embark on the final leg of the journey with the Titanic dataset. Your mission is to train a Logistic Regression model to predict survival. You'll preprocess the data, split it, standardize the features, and then train and evaluate your model. Ready for liftoff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c461ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score  # Changed from precision_score to accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Drop columns with strings and 'pclass', which is categorical but read as a numeric type\n",
    "titanic_df = titanic_df.select_dtypes(exclude=['object', 'category'])\n",
    "\n",
    "# Handle any NaN values by filling them with the mean of the column (ignoring 'pclass', which is categorical)\n",
    "numeric_columns = titanic_df.columns.drop('pclass')\n",
    "titanic_df[numeric_columns] = titanic_df[numeric_columns].fillna(titanic_df[numeric_columns].mean())\n",
    "\n",
    "# Convert 'pclass' to integer type if it's not already\n",
    "titanic_df['pclass'] = titanic_df['pclass'].astype(int)\n",
    "\n",
    "# TODO: Split the dataset into training and testing sets with a 70%-30% split\n",
    "\n",
    "# TODO: Identify and separate the target variable 'survived' from the training and testing data\n",
    "\n",
    "# TODO: Initialize StandardScaler and scale the features\n",
    "\n",
    "# TODO: Initialize the Logistic Regression model and train it on the scaled training data\n",
    "\n",
    "# TODO: Use the trained model to make predictions on the scaled testing data\n",
    "\n",
    "# TODO: Calculate and print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21985d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
